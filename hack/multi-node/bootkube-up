#!/bin/bash
set -euo pipefail

un="$(uname)"
local_os="linux"
if [ ${un} == 'Darwin' ]; then
    local_os="darwin"
fi

# Render assets
if [ ! -d "cluster" ]; then
  ../../bin/${local_os}/bootkube render --asset-dir=cluster --api-servers=https://172.17.4.101:443 --etcd-servers=http://172.17.4.51:2379
fi

# Add rendered kubeconfig to the node user-data
cat user-data.sample > cluster/user-data && sed 's/^/      /' cluster/auth/kubeconfig >> cluster/user-data
cp cluster/user-data{,-worker}
cp cluster/user-data{,-controller}
sed -i.bak -e '/--node-labels=master=true/d' cluster/user-data-worker

# Start the VM
vagrant up

ssh_ident="$(vagrant ssh-config c1 | awk '/IdentityFile/ {print $2}' | tr -d '"')"
ssh_port="$(vagrant ssh-config c1 | awk '/Port [0-9]+/ {print $2}')"

# Copy locally rendered assets to the server
scp -q -o stricthostkeychecking=no -i ${ssh_ident} -P ${ssh_port} -r cluster core@127.0.0.1:/home/core/cluster
scp -q -o stricthostkeychecking=no -i ${ssh_ident} -P ${ssh_port} ../../bin/linux/bootkube core@127.0.0.1:/home/core

# Run bootkube
ssh -q -o stricthostkeychecking=no -i ${ssh_ident} -p ${ssh_port} core@127.0.0.1 "sudo /home/core/bootkube start --asset-dir=/home/core/cluster --etcd-server=http://172.17.4.51:2379"

echo
echo "Bootstrap complete. Access your kubernetes cluster using:"
echo "kubectl --kubeconfig=cluster/auth/kubeconfig get nodes"
echo
