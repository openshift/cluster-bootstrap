#!/bin/bash
set -euo pipefail

un="$(uname)"
local_os="linux"
if [ ${un} == 'Darwin' ]; then
    local_os="darwin"
fi

SELF_HOST_ETCD=${SELF_HOST_ETCD:-false}
if [ ${SELF_HOST_ETCD} = "true" ]; then
    echo "WARNING: THIS IS NOT YET FULLY WORKING - merely here to make ongoing testing easier"
    etcd_render_flags="--etcd-servers=http://10.3.0.15:2379 --storage-backend=etcd3 --experimental-self-hosted-etcd"
    etcd_start_flags="--etcd-server=http://172.17.4.101:12379 --experimental-self-hosted-etcd"
else
    etcd_render_flags="--etcd-servers=http://172.17.4.51:2379"
    etcd_start_flags="--etcd-server=http://172.17.4.51:2379"
fi

# Render assets
if [ ! -d "cluster" ]; then
  ../../_output/bin/${local_os}/bootkube render --asset-dir=cluster --api-servers=https://172.17.4.101:443 ${etcd_render_flags}
  # Add rendered kubeconfig to the node user-data
  cat user-data.sample > cluster/user-data && sed 's/^/      /' cluster/auth/kubeconfig >> cluster/user-data
  cp cluster/user-data{,-worker}
  cp cluster/user-data{,-controller}
  sed -i.bak -e '/--node-labels=master=true/d' cluster/user-data-worker
fi

# Start the VM
vagrant up
vagrant ssh-config c1 > ssh_config

# Copy locally rendered assets to the server
scp -q -F ssh_config -r cluster core@c1:/home/core/cluster
scp -q -F ssh_config ../../_output/bin/linux/bootkube core@c1:/home/core

# Run bootkube
ssh -q -F ssh_config core@c1 "sudo /home/core/bootkube start --asset-dir=/home/core/cluster ${etcd_start_flags} 2>> /home/core/bootkube.log"

echo
echo "Bootstrap complete. Access your kubernetes cluster using:"
echo "kubectl --kubeconfig=cluster/auth/kubeconfig get nodes"
echo
